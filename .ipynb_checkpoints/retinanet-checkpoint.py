import torch.nn as nn
import torch
import numpy as np
from utils import ConvBlock, BasicBlock, Bottleneck
from anchors import RPN
from losses import FocalLoss

import sys, os


def layers_dim(h_x, w_x, factor, n_layers):
    h = []
    w = []
    for i in range(n_layers):
        h.append(h_x)
        w.append(w_x)
        h_x /= float(factor)
        w_x /= float(factor)
    return h, w



class FPN(nn.Module):
    """ 
    Computes features throughout varying levels of the network.
    Contains adjustments as per RetinaNet paper; a) P6 obtained
    via 3x3 stride 2 convolution on C5; b) P7 computed by applying
    ReLU followed by a 3x3 stride 2 convolution on P6.
    
    Input:
        - features : list of output features at each level of the forward pass in decreasing order (C5, C4, etc.)
        - dimensions : list of feature map dimensions at each level of the forward pass in decreasing order.
        - max_plane : dimension of the largest feature map (last at the top of the network)
        - feature_size : features at each level of the backward pass (all have same fixed length)
        
    Output:
        - p_results : output values at each of the layers in descending order
    
    """
    
    # We use constant output features, although we can use each of the channels to preserve lenght & scales
    def __init__(self, features, dimensions, feature_size=256):
        super(FPN, self).__init__()
        assert len(features) == len(dimensions), "Lateral connections do not match top-to-bottom connections."
        
        self.feature_size = feature_size
        # 
        self.n_layers = len(features)
        

        # Operations
        self.conv1 = self._make_conv(features, kernel=1, stride=1, padding=0)
        self.upsampler = self._make_upsampler(dimensions)
        self.conv2 = self._make_conv([feature_size] * self.n_layers, kernel=3, stride=1, padding=1)
        
        # Operations P6 & P7
        self.p6 = nn.Conv2d(features[-1], feature_size, kernel_size=3, stride=2, padding=1)
        self.p7 = nn.Sequential(
            nn.ReLU(),
            nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=2, padding=1)
        )
        
        
        
    def _make_conv(self, features, kernel, stride, padding):
        """ Creates modules with convolutions for each layer."""
        layer = []
        for i in range(len(features)):
            layer.insert(0, nn.Conv2d(features[i], self.feature_size, kernel_size=kernel, stride=stride, padding=padding) )
            
        return nn.ModuleList(layer)
        
        
    def _make_upsampler(self, dimensions):
        """ Creates upsampler modules for each level."""
        layer = []
        for i in range(len(dimensions) - 1):
            layer.insert(0, nn.Upsample(size=(int(dimensions[i])), mode='bilinear', align_corners=False) )
            
        return nn.ModuleList(layer)
            
        
    def forward(self, x):
        p_results = []
        
        # There is no fusion on 1st layer
        x_lateral = self.conv1[0](x[0])
        x_out = self.conv2[0](x_lateral)
    
        p_results.append(x_out)
        
        for i in range(1, self.n_layers):
            # 1. Compute top input 
            x_top = self.upsampler[i-1](x_lateral)
            
            # 2. Retrieve lateral input
            x_lateral = self.conv1[i](x[i])
            
            # 3. Merge lateral and top input
            x_merged = x_lateral + x_top
            
            # 4. Compute PX output
            x_out = self.conv2[i](x_merged)
            
            # 5. Retain results
            p_results.append(x_out)
        
        # 6. Compute P6 & P7
        p6 = self.p6(x[0])
        p7 = self.p7(p6)
        p_results.insert(0, p6)
        p_results.insert(0, p7)
        
        
        # Need to verify that when results are added in array, information of the gradient is still not lost...
        print(">>> P7:", p_results[0].size())
        print(">>> P6:", p_results[1].size()) 
        print(">>> P5:", p_results[2].size())
        print(">>> P4:", p_results[3].size())
        print(">>> P3:", p_results[4].size())
        
        return p_results

    
class RegressionModel(nn.Module):
    """
    Input:
        - in_plane : features generated by FPN model at a given level.
    """
    
    def __init__(self, in_plane, n_layers=4, num_anchors=9, feature_size=256):
        super(RegressionModel, self).__init__()
        
        in_features = [in_plane] + [feature_size] * (n_layers-1)
        
        self.convgroup = self._make_convgroup(n_layers, in_features, feature_size)
        self.output = nn.Conv2d(feature_size, num_anchors * 4, kernel_size=3, stride=1, padding=1)
        

    def _make_convgroup(self, n_layers, in_features, out_features):
        layers = []
        for i in range(n_layers):
            layers.append( nn.Conv2d(in_features[i], out_features, kernel_size=3, padding=1) )
            layers.append( nn.ReLU() )
            
        return nn.Sequential(*layers)

    def forward(self, x):
        out = self.convgroup(x)
        out = self.output(out)
        
        return out
        
        

class ClassificationModel(nn.Module):
    
    def __init__(self, in_plane, n_layers=4, num_anchors=9, num_classes=80, prior=0.01, feature_size=256):
        super(ClassificationModel, self).__init__()
        
        in_features = [in_plane] + [feature_size] * (n_layers-1)
        self.num_anchors = num_anchors
        self.num_classes = num_classes
        
        self.convgroup = self._make_convgroup(n_layers, in_features, feature_size)
        self.output = nn.Conv2d(feature_size, num_anchors * num_classes, kernel_size=3, stride=1, padding=1)
        self.output_act = nn.Sigmoid()
        
        
    def _make_convgroup(self, n_layers, in_features, out_features):
        layers = []
        for i in range(n_layers):
            layers.append( nn.Conv2d(in_features[i], out_features, kernel_size=3, padding=1) )
            layers.append( nn.ReLU() )
            
        return nn.Sequential(*layers)
        
    
    def forward(self, x):
        out = self.convgroup(x)
        out = self.output(out)
        out = self.output_act(out)
        # Output: [N,C,H,W], with C = num_classes * num_anchors
        
        out1 = out.permute(0, 2, 3, 1)
        N, H, W, C = out1.size()
        out2 = out1.view(N, H, W, self.num_anchors, self.num_classes)
        
        # [N, HxWxnum_anchors, num_classes]
        out3 = out2.contiguous().view(x.size(0), -1, self.num_classes)
        
        return out3
        
    
    
class RetinaNet(nn.Module):
    """ 
    Ideally, it should extend to allow more multiple type of backbones.
    """
    
    def __init__(self,):
        super(RetinaNet, self).__init__()
        
    def forward(self):
        pass
        

        
class ResNet(nn.Module):
    
    def __init__(self, x_dim, layers, block, num_classes):
        super(ResNet, self).__init__()
        
        self.training = True
        
        self.in_plane = 64
        
        # Compute H & W at each layer
        x_dim = x_dim / 4.
        h_x, w_x = layers_dim( x_dim[0], x_dim[1], factor=2, n_layers=len(layers))
        
        self.conv1 = ConvBlock(3, 64, kernel=7, stride=2, pad=3, bias=False)
        
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        # Needs to be converted toa for loop
        self.conv_2 = self._make_layers(block, layers[0], 64)        
        self.conv_3 = self._make_layers(block, layers[1], 128, stride=2)
        self.conv_4 = self._make_layers(block, layers[2], 256, stride=2)
        self.conv_5 = self._make_layers(block, layers[3], 512, stride=2)
        
        # Feature Pyramid Network
        self.fpn = FPN([128, 256, 512], h_x[1:])
        
        # Regression Model
        self.regressionModel = RegressionModel(256)
        self.classificationModel = ClassificationModel(256, num_classes=num_classes)
        self.anchors = RPN()
        
#         self.avgpool = nn.AvgPool2d(kernel_size=self.conv_5[layers[-1]])
#         self.fc = nn.Linear(512 * block.expansion, num_classes)
        
        
    def _make_layers(self, block, n_layers, out_plane, stride=1):
        
        downsample = None
        
        # Assume downsampling is performed by adding an extra layer to perform downsampling
        if self.in_plane != out_plane or block == Bottleneck:
            downsample = ConvBlock(self.in_plane, out_plane * block.expansion, kernel=1, stride=stride, pad=0, bias=False, is_relu=False)
        
        layers = [ block(self.in_plane, out_plane, stride, downsample) ]
        self.in_plane = out_plane * block.expansion
        for i in range(1, n_layers):
            layers.append( block(self.in_plane, out_plane) )
            
        return nn.Sequential(*layers)
        
        
    def forward(self, img_batch):
        
        c_outputs = []
        
        x = self.conv1(img_batch)
        x = self.maxpool(x)
        
        # This could be for-loop
        x1 = self.conv_2(x)
        x2 = self.conv_3(x1)
        x3 = self.conv_4(x2)
        x4 = self.conv_5(x3)
        
        # Reverse order for top-to-bottom pass
        features = self.fpn([x4,x3,x2])
        
        # Regression-Box (Different): list of varying sizes
        regression = [self.regressionModel(feature) for feature in features]
        
        # Classification
        classification = [self.classificationModel(feature) for feature in features]
        
        # Anchors
        anchors = self.anchors(img_batch)
        
        if self.training:
            return self.focalLoss(classification, regression, anchors, annotations)
        
        return features
    
    
if __name__ == '__main__':
    print(">>> Start...")
    
    x = torch.randn((10, 3, 224, 224))
    x_dim = np.array([x.size(2), x.size(3)])
    layers = [2, 3, 6, 4]
    
    model = ResNet(x_dim, layers, BasicBlock, 1000)
    model.forward(x)